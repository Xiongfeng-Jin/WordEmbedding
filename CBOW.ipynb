{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CBOW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xiongfeng-Jin/WordEmbedding/blob/master/CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x98Gcam1tZSv",
        "colab_type": "code",
        "outputId": "ac3055a0-6711-42e1-e112-d1d0c16c8813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import bz2\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk # standard preprocessing\n",
        "import operator # sorting items in dictionary by value\n",
        "nltk.download('punkt') #tokenizers/punkt/PY3/english.pickle\n",
        "from math import ceil"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDa-9Q6cCyve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6RGFIIKt17e",
        "colab_type": "text"
      },
      "source": [
        "Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIaUzD_2ttB0",
        "colab_type": "code",
        "outputId": "ddff3539-26c1-45e8-b9b2-8bfef92bf652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "url = 'http://www.evanjones.ca/software/'\n",
        "\n",
        "def download_if_needed(filename,expected_bytes):\n",
        "  if not os.path.exists(filename):\n",
        "    print(\"Downloading file...\")\n",
        "    filename,_ = urlretrieve(url+filename,filename)\n",
        "  statinfo = os.stat(filename)\n",
        "  if statinfo.st_size == expected_bytes:\n",
        "    print(\"Found and verified %s\" % filename)\n",
        "  else:\n",
        "    print(statinfo.st_size)\n",
        "    raise Exception(\"Failed to verify\"+filename)\n",
        "  return filename\n",
        "\n",
        "filename = download_if_needed('wikipedia2text-extracted.txt.bz2', 18377035)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified wikipedia2text-extracted.txt.bz2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEIiQodwutYJ",
        "colab_type": "code",
        "outputId": "5f541120-a5e0-4315-de67-9b6d77abb2b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def read_data(filename):\n",
        "  with bz2.BZ2File(filename) as f:\n",
        "    data = []\n",
        "    file_string = f.read().decode('utf-8')\n",
        "    file_string = nltk.word_tokenize(file_string)\n",
        "    data.extend(file_string)\n",
        "  return data\n",
        "\n",
        "words = read_data(filename)\n",
        "print(\"data size %d\" % len(words))\n",
        "print(\"Example words: %s\" % words[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data size 11631723\n",
            "Example words: ['Propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXXbLgDWxBYN",
        "colab_type": "text"
      },
      "source": [
        "# Build word dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hha56p-xvTmg",
        "colab_type": "code",
        "outputId": "d1994949-d18b-4287-aabe-da19587f3a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "vocabulary_size = 50000\n",
        "\n",
        "def build_dataset(words):\n",
        "  count = [['UNK',-1]]\n",
        "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
        "  dictionary = dict()\n",
        "  i = 0\n",
        "  for word,_ in count:\n",
        "    dictionary[word] = i\n",
        "    i += 1\n",
        "  \n",
        "  data = []\n",
        "  unk_count = 0\n",
        "  for word in words:\n",
        "    if word in dictionary:\n",
        "      index = dictionary[word]\n",
        "    else:\n",
        "      index = 0\n",
        "      unk_count += 1\n",
        "    data.append(index)\n",
        "    \n",
        "  count[0][1] = unk_count\n",
        "  reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
        "  assert len(dictionary) == vocabulary_size\n",
        "  return data,count,dictionary,reverse_dictionary\n",
        "\n",
        "data,count,dictionary,reverse_dictionary = build_dataset(words)\n",
        "print(\"most common words\",count[:5])\n",
        "print(\"sample data\",data[:10])\n",
        "# del words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "most common words [['UNK', 388121], ('the', 690296), (',', 632179), ('.', 439932), ('of', 402970)]\n",
            "sample data [18392, 9, 8, 19083, 221, 4, 6436, 3769, 30, 12058]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhg8_Lka0fvJ",
        "colab_type": "text"
      },
      "source": [
        "## Generating Batches of Data for Skip-Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA7yYAQh0A-B",
        "colab_type": "code",
        "outputId": "dcfeb14a-54d3-4569-c0d6-296cbf966fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "data_index = 0\n",
        "def generate_batch_cbow(batch_size,window_size):\n",
        "  global data_index\n",
        "  \n",
        "  span = 2*window_size + 1\n",
        "  \n",
        "  batch = np.ndarray(shape=(batch_size,span-1),dtype=np.int32)\n",
        "  labels = np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
        "  \n",
        "  buffer = collections.deque(maxlen=span)\n",
        "  dataLen = len(data)\n",
        "  for _ in range(span):\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % dataLen\n",
        "    \n",
        "  num_samples = 2 * window_size\n",
        "  \n",
        "  for i in range(batch_size):\n",
        "    target = window_size\n",
        "    target_to_avoid = [window_size]\n",
        "    col_idx = 0\n",
        "    for j in range(span):\n",
        "      if j == span//2:\n",
        "        continue\n",
        "      batch[i,col_idx] = buffer[j]\n",
        "      col_idx += 1\n",
        "    labels[i,0] = buffer[target]\n",
        "    \n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % dataLen\n",
        "\n",
        "  return batch,labels\n",
        "\n",
        "for window_size in [1,2]:\n",
        "  data_index = 0\n",
        "  batch,labels = generate_batch_cbow(batch_size=8,window_size=window_size)\n",
        "  print(\"window size:\",window_size)\n",
        "  print(\"batch:\",[[reverse_dictionary[bii] for bii in bi] for bi in batch])\n",
        "  print(\"labels:\", [reverse_dictionary[li] for li in labels.reshape(8)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "window size: 1\n",
            "batch: [['Propaganda', 'a'], ['is', 'concerted'], ['a', 'set'], ['concerted', 'of'], ['set', 'messages'], ['of', 'aimed'], ['messages', 'at'], ['aimed', 'influencing']]\n",
            "labels: ['is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at']\n",
            "window size: 2\n",
            "batch: [['Propaganda', 'is', 'concerted', 'set'], ['is', 'a', 'set', 'of'], ['a', 'concerted', 'of', 'messages'], ['concerted', 'set', 'messages', 'aimed'], ['set', 'of', 'aimed', 'at'], ['of', 'messages', 'at', 'influencing'], ['messages', 'aimed', 'influencing', 'the'], ['aimed', 'at', 'the', 'opinions']]\n",
            "labels: ['a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILJVVcnU3_BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "embedding_size = 128\n",
        "window_size = 2\n",
        "valid_size = 16\n",
        "valid_window = 50\n",
        "valid_examples = np.array(random.sample(range(valid_window),valid_size))\n",
        "valid_examples = np.append(valid_examples,random.sample(range(1000,1000+valid_window),valid_size),axis=0)\n",
        "num_sampled = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kX-Flv6Bh4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
        "softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev=0.5/math.sqrt(embedding_size)))\n",
        "softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAkavX_ZBnbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stacked_embedings(train_dataset):\n",
        "  for i in range(2*window_size):\n",
        "    embedding_i = tf.nn.embedding_lookup(embeddings, train_dataset[:,i])\n",
        "    x_size,y_size = embedding_i.get_shape().as_list()\n",
        "    temp = None\n",
        "    if temp is None:\n",
        "        temp = tf.reshape(embedding_i,[x_size,y_size,1])\n",
        "    else:\n",
        "        temp = tf.concat(axis=2,values=[temp,tf.reshape(embedding_i,[x_size,y_size,1])])\n",
        "  return temp\n",
        "\n",
        "def mean_embeddings(x):\n",
        "  return tf.reduce_mean(stacked_embedings(x),2,keepdims=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBJy-8MZDxMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(x,y):\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.sampled_softmax_loss(\n",
        "        weights=softmax_weights,\n",
        "        biases=softmax_biases,\n",
        "        inputs=mean_embeddings(x),\n",
        "        labels=y,\n",
        "        num_sampled=num_sampled,\n",
        "        num_classes=vocabulary_size\n",
        "    )\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBx8y0I2EgSk",
        "colab_type": "text"
      },
      "source": [
        "### Calculating word similarities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbHix3MAEWGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_normalized_embeddings():\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keepdims=True))\n",
        "  normalized_embeddings = embeddings / norm\n",
        "  return normalized_embeddings\n",
        "\n",
        "def get_similarity():\n",
        "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "  normalized_embeddings = get_normalized_embeddings()\n",
        "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
        "  similarity = tf.matmul(valid_embeddings,tf.transpose(normalized_embeddings))\n",
        "  return similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZhTqTxfFDTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdagradOptimizer(1.0)\n",
        "def train(x,y):\n",
        "  optimizer.minimize(lambda:loss(x,y),var_list=[embeddings,softmax_weights,softmax_biases])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbJJoVKgFckJ",
        "colab_type": "code",
        "outputId": "ec3b4a23-e927-43e3-b3a4-fa2561f604ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3029
        }
      },
      "source": [
        "num_steps = 100001\n",
        "skip_losses = []\n",
        "for step in range(num_steps):\n",
        "  batch_data,batch_labels = generate_batch_cbow(batch_size,window_size)\n",
        "  batch_data = tf.constant(batch_data,dtype=tf.int32)\n",
        "  batch_labels = tf.constant(batch_labels,dtype=tf.int32)\n",
        "  train(batch_data,batch_labels)\n",
        "  if (step+1) % 10000 == 0:\n",
        "    sim = get_similarity()\n",
        "    for i in range(valid_size):\n",
        "      valid_word = reverse_dictionary[valid_examples[i]]\n",
        "      top_k = 8\n",
        "      nearest = (-sim[i,:]).numpy().argsort()[1:top_k+1]\n",
        "      log = \"Nearest to %s: \" % valid_word\n",
        "      for k in range(top_k):\n",
        "        close_word = reverse_dictionary[nearest[k]]\n",
        "        log = \"%s %s,\" % (log,close_word)\n",
        "      print(log)\n",
        "    print(\"-\"*100)\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "Nearest to are:  were, is, have, has, was, CDMA, can, 21.3,\n",
            "Nearest to at:  in, on, with, to, ., pharaoh, through, Superboy,\n",
            "Nearest to on:  in, with, at, to, for, from, by, Jacques,\n",
            "Nearest to by:  to, with, as, from, on, in, allied, synod,\n",
            "Nearest to 's:  day, Barnes, difference, lover, ratings, name, fisherman, Tao,\n",
            "Nearest to have:  be, are, outlook, 1655, journal, Eduard, Minoan, Wait,\n",
            "Nearest to but:  In, The, 420,000, which, exuberant, Bangladeshis, including, reflective,\n",
            "Nearest to a:  an, the, this, his, any, one, Brand, rewrites,\n",
            "Nearest to which:  The, In, and, but, Chaffee, Dares, It, patience,\n",
            "Nearest to has:  is, was, are, genre, Assent, emerged, supposedly, prefect,\n",
            "Nearest to with:  to, by, on, for, at, in, Bien, Oahu,\n",
            "Nearest to were:  are, may, was, breakage, will, Wax, would, Automatic,\n",
            "Nearest to his:  the, seafarers, a, this, psalm, perversion, sub-species, closest,\n",
            "Nearest to as:  by, Anscombe, ``, Sigma, smaller, Moray, Sharjah, refined,\n",
            "Nearest to or:  grief, bay, rehearsed, Yavana, Laws, Daneel, quagmire, Melrose,\n",
            "Nearest to this:  the, a, predominant, his, an, Hurst, these, intoxicated,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  were, is, have, can, has, was, had, include,\n",
            "Nearest to at:  on, in, with, for, to, ., by, while,\n",
            "Nearest to on:  in, at, with, for, from, against, to, by,\n",
            "Nearest to by:  with, as, to, in, from, on, into, at,\n",
            "Nearest to 's:  fisherman, Kingdom, lover, name, Barnes, athlete, thoroughbred, day,\n",
            "Nearest to have:  are, be, were, journal, had, 1655, outlook, biosynthetic,\n",
            "Nearest to but:  In, The, However, It, which, This, including, According,\n",
            "Nearest to a:  an, the, one, this, his, its, some, Rees,\n",
            "Nearest to which:  The, and, In, but, Chaffee, This, It, However,\n",
            "Nearest to has:  is, was, also, are, can, had, may, were,\n",
            "Nearest to with:  for, by, at, on, to, in, sequel, over,\n",
            "Nearest to were:  are, have, had, was, would, will, may, is,\n",
            "Nearest to his:  the, her, this, their, seafarers, perversion, its, a,\n",
            "Nearest to as:  by, ``, Magic, result, cricketers, halogens, discounted, Cooper,\n",
            "Nearest to or:  and, bay, depletion, android, vaults, Stadt, Milland, .,\n",
            "Nearest to this:  the, his, these, predominant, an, a, Bund, quarantine,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  were, is, can, has, have, may, such, include,\n",
            "Nearest to at:  on, in, with, for, from, against, during, into,\n",
            "Nearest to on:  in, at, for, with, against, from, to, by,\n",
            "Nearest to by:  with, as, for, to, into, on, in, Jan,\n",
            "Nearest to 's:  fisherman, Kingdom, Barnes, Tao, day, flushing, thoroughbred, d,\n",
            "Nearest to have:  be, are, were, biosynthetic, Eduard, had, journal, obscured,\n",
            "Nearest to but:  In, The, It, However, As, This, which, He,\n",
            "Nearest to a:  an, the, one, his, various, their, Wilmot, Rees,\n",
            "Nearest to which:  but, The, It, Chaffee, This, In, while, There,\n",
            "Nearest to has:  is, was, also, are, can, may, had, will,\n",
            "Nearest to with:  on, at, by, for, against, between, from, inductance,\n",
            "Nearest to were:  are, had, was, have, did, would, began, is,\n",
            "Nearest to his:  her, their, this, the, Suisse, a, seafarers, its,\n",
            "Nearest to as:  by, result, cricketers, halogens, including, Secretariat, Former, Belorussian,\n",
            "Nearest to or:  ., (, ;, depletion, and, android, which, UNK,\n",
            "Nearest to this:  these, the, his, some, predominant, Bund, Sapieha, D2,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  were, can, is, have, has, may, was, will,\n",
            "Nearest to at:  in, on, during, with, over, against, into, for,\n",
            "Nearest to on:  in, at, against, for, to, into, by, among,\n",
            "Nearest to by:  into, with, only, to, as, on, at, in,\n",
            "Nearest to 's:  fisherman, athlete, ’, ratings, day, intolerable, Kingdom, stupidity,\n",
            "Nearest to have:  are, were, be, had, Eduard, niche, biosynthetic, include,\n",
            "Nearest to but:  In, The, It, However, This, He, There, which,\n",
            "Nearest to a:  an, the, some, one, several, Hispania, been, any,\n",
            "Nearest to which:  but, It, In, This, or, The, where, Chaffee,\n",
            "Nearest to has:  is, was, are, also, can, may, will, does,\n",
            "Nearest to with:  by, at, sequel, in, to, on, after, ripped,\n",
            "Nearest to were:  are, had, was, have, would, did, began, may,\n",
            "Nearest to his:  her, the, their, this, its, them, him, perversion,\n",
            "Nearest to as:  result, by, discounted, LED, cricketers, biochemistry, Belorussian, Tidbinbilla,\n",
            "Nearest to or:  and, android, but, which, ;, ., vaults, without,\n",
            "Nearest to this:  the, these, some, his, it, Bund, predominant, an,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  is, can, were, has, have, may, had, also,\n",
            "Nearest to at:  on, in, during, against, over, into, for, through,\n",
            "Nearest to on:  at, in, for, against, during, into, spirited, omnivores,\n",
            "Nearest to by:  into, as, in, only, with, after, at, on,\n",
            "Nearest to 's:  ’, d, fisherman, Kingdom, day, Alexandre, has, uneven,\n",
            "Nearest to have:  are, were, be, had, niche, include, Eduard, biosynthetic,\n",
            "Nearest to but:  The, In, It, He, However, As, while, This,\n",
            "Nearest to a:  an, the, some, several, one, another, any, many,\n",
            "Nearest to which:  where, while, but, It, based, and, Chaffee, Most,\n",
            "Nearest to has:  is, was, also, are, can, may, will, had,\n",
            "Nearest to with:  to, sequel, by, Breen, when, at, Bien, after,\n",
            "Nearest to were:  had, are, was, have, did, began, would, came,\n",
            "Nearest to his:  her, the, this, its, their, them, him, he,\n",
            "Nearest to as:  by, result, 280,000, ``, halogens, biochemistry, subtext, well,\n",
            "Nearest to or:  Mandana, android, coherently, ;, ., and, space, aesthetics,\n",
            "Nearest to this:  the, these, his, some, it, its, Sapieha, predominant,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  were, is, can, has, may, have, was, had,\n",
            "Nearest to at:  on, during, in, when, over, with, for, against,\n",
            "Nearest to on:  in, at, MAC, by, with, Bayfront, for, 820,\n",
            "Nearest to by:  into, only, on, in, as, with, at, during,\n",
            "Nearest to 's:  ’, fisherman, d, launcher, Kingdom, has, Alexandre, DM,\n",
            "Nearest to have:  be, are, were, had, Eduard, include, also, still,\n",
            "Nearest to but:  In, However, The, It, A, He, while, As,\n",
            "Nearest to a:  an, the, one, some, several, another, many, little,\n",
            "Nearest to which:  It, where, The, There, and, However, This, A,\n",
            "Nearest to has:  is, was, also, can, are, may, does, will,\n",
            "Nearest to with:  when, at, during, on, up, by, Bien, after,\n",
            "Nearest to were:  are, had, was, did, would, began, could, can,\n",
            "Nearest to his:  her, the, this, their, these, them, its, unseeded,\n",
            "Nearest to as:  result, by, plagues, biochemistry, halogens, uncontrollable, discounted, magnetized,\n",
            "Nearest to or:  android, and, Mandana, Jayuya, Geller, stigmas, but, Symbolism,\n",
            "Nearest to this:  these, the, his, it, some, there, predominant, Sapieha,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  can, were, is, may, has, was, have, will,\n",
            "Nearest to at:  on, in, during, for, before, by, off, against,\n",
            "Nearest to on:  at, in, upon, Bayfront, against, for, ripped, demographics,\n",
            "Nearest to by:  into, after, only, at, as, with, using, identical,\n",
            "Nearest to 's:  ’, fisherman, launcher, ratings, Iguaçu, charts, athlete, Alexandre,\n",
            "Nearest to have:  be, are, were, had, may, Eduard, can, include,\n",
            "Nearest to but:  In, As, However, It, This, A, while, He,\n",
            "Nearest to a:  an, the, one, some, several, another, Hispania, its,\n",
            "Nearest to which:  where, It, while, Chaffee, based, well, and, There,\n",
            "Nearest to has:  is, was, also, are, may, can, does, will,\n",
            "Nearest to with:  after, by, against, ripped, at, when, on, wagon,\n",
            "Nearest to were:  are, had, was, did, would, began, became, could,\n",
            "Nearest to his:  her, their, this, its, unseeded, them, these, the,\n",
            "Nearest to as:  result, by, biochemistry, 280,000, including, halogens, Belorussian, airships,\n",
            "Nearest to or:  and, android, Toleration, but, without, Symbolism, 1158, circumcision,\n",
            "Nearest to this:  these, the, his, some, it, Bund, their, what,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  were, can, may, is, have, include, has, was,\n",
            "Nearest to at:  in, on, during, ., against, with, through, throughout,\n",
            "Nearest to on:  in, at, against, Bayfront, over, upon, along, MAC,\n",
            "Nearest to by:  as, only, nematode, because, in, Jan, identical, after,\n",
            "Nearest to 's:  ’, fisherman, shoppers, Kingdom, Iguaçu, Succession, launcher, Carlsbad,\n",
            "Nearest to have:  are, be, were, Eduard, include, may, exist, had,\n",
            "Nearest to but:  In, However, This, It, The, As, although, He,\n",
            "Nearest to a:  an, several, one, the, another, some, various, any,\n",
            "Nearest to which:  where, based, whose, and, It, well, Chaffee, while,\n",
            "Nearest to has:  is, was, can, does, had, also, may, are,\n",
            "Nearest to with:  when, during, at, after, through, along, against, on,\n",
            "Nearest to were:  are, had, was, did, would, began, have, can,\n",
            "Nearest to his:  her, their, this, its, them, the, these, himself,\n",
            "Nearest to as:  result, by, including, discounted, biochemistry, Sigma, Sint, uncontrollable,\n",
            "Nearest to or:  ;, android, but, without, characterises, Toleration, archipelagos, reeds,\n",
            "Nearest to this:  these, the, his, some, it, what, Bund, intoxicated,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  can, were, may, is, have, did, will, include,\n",
            "Nearest to at:  in, on, during, over, around, through, about, with,\n",
            "Nearest to on:  in, at, MAC, upon, Bayfront, held, against, TCP/IP,\n",
            "Nearest to by:  Jan, as, into, nematode, using, identical, according, with,\n",
            "Nearest to 's:  ’, fisherman, ratings, shoppers, Iguaçu, Carlsbad, thoroughbred, athlete,\n",
            "Nearest to have:  are, be, exist, were, Eduard, do, had, belong,\n",
            "Nearest to but:  However, In, The, although, while, A, This, As,\n",
            "Nearest to a:  an, the, another, one, adjust, Tupelo, several, little,\n",
            "Nearest to which:  based, It, while, where, Chaffee, However, called, headed,\n",
            "Nearest to has:  is, was, can, may, does, also, had, are,\n",
            "Nearest to with:  case, against, at, when, wagon, journeys, along, Spice,\n",
            "Nearest to were:  are, had, would, did, was, took, began, could,\n",
            "Nearest to his:  her, their, himself, its, the, them, this, Suisse,\n",
            "Nearest to as:  107, Sigma, result, by, including, uncontrollable, plagues, discounted,\n",
            "Nearest to or:  and, ;, android, characterises, Geller, ., epigraph, extramarital,\n",
            "Nearest to this:  these, the, some, it, Bund, predominant, any, what,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Nearest to are:  can, were, is, may, have, did, does, has,\n",
            "Nearest to at:  during, in, on, while, tabletop, over, ., when,\n",
            "Nearest to on:  in, at, MAC, upon, Bayfront, against, d'affaires, /k/,\n",
            "Nearest to by:  atman, using, nematode, identical, advocacy, compression, with, 275,\n",
            "Nearest to 's:  ’, fisherman, ratings, thoroughbred, Alexandre, Barnes, Kingdom, VIA,\n",
            "Nearest to have:  are, be, were, exist, Eduard, had, possess, biosynthetic,\n",
            "Nearest to but:  However, In, The, while, although, They, As, A,\n",
            "Nearest to a:  an, the, another, one, getting, several, bureaucracies, any,\n",
            "Nearest to which:  based, Chaffee, where, It, whose, headed, but, consisting,\n",
            "Nearest to has:  is, was, may, can, does, also, consists, will,\n",
            "Nearest to with:  Breen, journeys, case, wagon, Epistle, -200, McCauley, priesthood,\n",
            "Nearest to were:  are, would, had, was, did, came, could, have,\n",
            "Nearest to his:  her, the, its, their, seafarers, them, himself, Suisse,\n",
            "Nearest to as:  result, cricketers, 107, including, Secretariat, Sigma, treated, halogens,\n",
            "Nearest to or:  android, Differences, coherently, quagmire, uncomfortable, seeming, OR, extramarital,\n",
            "Nearest to this:  these, predominant, the, it, intoxicated, Bund, there, solidify,\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUN3rtJwik0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}